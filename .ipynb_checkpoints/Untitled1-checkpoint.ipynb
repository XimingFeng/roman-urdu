{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, word2idx = urdu.get_unique_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of unique words  31149\n"
     ]
    }
   ],
   "source": [
    "print(\"total amount of unique words \", len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unkwn', 'sai', 'kha', 'ya', 'her']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[0: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data entry number:  20229\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = urdu.get_processed_data(max_sent_len, word_list, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20229, 20)\n",
      "(20229,)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 64\n",
    "num_classes = 3\n",
    "lstm_hidden_dim = 64\n",
    "rnn_hidden_dim = 64\n",
    "batch_size = 128\n",
    "dense_dim = 64\n",
    "learn_rate = 0.005\n",
    "steps = 2000\n",
    "total_word_num = len(word_list)\n",
    "verbo_every = 100\n",
    "epochs = 10\n",
    "keep_prob = 0.5\n",
    "rnn_layers_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.int64, [None, max_sent_len])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "y_one_hot = tf.one_hot(y, depth=num_classes)\n",
    "embedding_lookup = tf.Variable(tf.random_normal([total_word_num, embed_size]))\n",
    "embed = tf.nn.embedding_lookup(params=embedding_lookup, ids=X)\n",
    "#lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_hidden_dim)\n",
    "#init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_hidden_dim)\n",
    "#lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "#                                          output_keep_prob=keep_prob)\n",
    "rnn_drop = tf.contrib.rnn.DropoutWrapper(rnn_cell, \n",
    "                                         output_keep_prob=keep_prob)\n",
    "# multi_layer_cells = tf.contrib.rnn.MultiRNNCell([lstm_drop] * rnn_layers_num)\n",
    "rnn_out, final_state = tf.nn.dynamic_rnn(rnn_drop, embed, dtype=tf.float32)\n",
    "#rnn_out, final_state = tf.nn.dynamic_rnn(lstm_drop, embed, dtype=tf.float32)\n",
    "dense_out = tf.contrib.layers.fully_connected(rnn_out[:, -1], \n",
    "                                              num_outputs=dense_dim, \n",
    "                                              activation_fn=tf.sigmoid)\n",
    "dense_drop = tf.contrib.layers.dropout(dense_out, keep_prob)\n",
    "scores = tf.contrib.layers.fully_connected(dense_drop, \n",
    "                                           num_outputs=num_classes, \n",
    "                                           activation_fn=None)\n",
    "y_pred = tf.nn.softmax_cross_entropy_with_logits(labels=y_one_hot, logits=scores)\n",
    "loss = tf.reduce_mean(y_pred)\n",
    "trainer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "matches = tf.equal(tf.argmax(scores, 1), y)\n",
    "acc = tf.reduce_mean(tf.cast(matches, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "val accuracy:  0.41\n",
      "test accuracy:  0.419179\n",
      "----------------------------------\n",
      "step:  100\n",
      "val accuracy:  0.55\n",
      "test accuracy:  0.545477\n",
      "----------------------------------\n",
      "step:  200\n",
      "val accuracy:  0.695\n",
      "test accuracy:  0.516312\n",
      "----------------------------------\n",
      "step:  300\n",
      "val accuracy:  0.71\n",
      "test accuracy:  0.49481\n",
      "----------------------------------\n",
      "step:  400\n",
      "val accuracy:  0.735\n",
      "test accuracy:  0.547454\n",
      "----------------------------------\n",
      "step:  500\n",
      "val accuracy:  0.77\n",
      "test accuracy:  0.556599\n",
      "----------------------------------\n",
      "step:  600\n",
      "val accuracy:  0.73\n",
      "test accuracy:  0.544488\n",
      "----------------------------------\n",
      "step:  700\n",
      "val accuracy:  0.785\n",
      "test accuracy:  0.55388\n",
      "----------------------------------\n",
      "step:  800\n",
      "val accuracy:  0.81\n",
      "test accuracy:  0.536332\n",
      "----------------------------------\n",
      "step:  900\n",
      "val accuracy:  0.78\n",
      "test accuracy:  0.555858\n",
      "----------------------------------\n",
      "step:  1000\n",
      "val accuracy:  0.795\n",
      "test accuracy:  0.562037\n",
      "----------------------------------\n",
      "step:  1100\n",
      "val accuracy:  0.805\n",
      "test accuracy:  0.54523\n",
      "----------------------------------\n",
      "step:  1200\n",
      "val accuracy:  0.79\n",
      "test accuracy:  0.580079\n",
      "----------------------------------\n",
      "step:  1300\n",
      "val accuracy:  0.745\n",
      "test accuracy:  0.568216\n",
      "----------------------------------\n",
      "step:  1400\n",
      "val accuracy:  0.82\n",
      "test accuracy:  0.566485\n",
      "----------------------------------\n",
      "step:  1500\n",
      "val accuracy:  0.78\n",
      "test accuracy:  0.549184\n",
      "----------------------------------\n",
      "step:  1600\n",
      "val accuracy:  0.835\n",
      "test accuracy:  0.567227\n",
      "----------------------------------\n",
      "step:  1700\n",
      "val accuracy:  0.755\n",
      "test accuracy:  0.567968\n",
      "----------------------------------\n",
      "step:  1800\n",
      "val accuracy:  0.82\n",
      "test accuracy:  0.568463\n",
      "----------------------------------\n",
      "step:  1900\n",
      "val accuracy:  0.82\n",
      "test accuracy:  0.562037\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step_idx in range(steps):\n",
    "        X_batch, y_batch = urdu.get_random_batch(X_train, y_train, batch_size)\n",
    "        for epoch_idx in range(epochs):\n",
    "            sess.run(trainer, feed_dict={X: X_batch, y: y_batch})\n",
    "        if step_idx % verbo_every == 0:\n",
    "            X_test_batch, y_test_batch = urdu.get_random_batch(X_test, y_test, 200)\n",
    "            X_val_batch, y_val_batch = urdu.get_random_batch(X_train, y_train, 200)\n",
    "            \n",
    "            test_acc = sess.run(acc, feed_dict={X: X_test, y: y_test})\n",
    "            val_acc = sess.run(acc, feed_dict={X: X_val_batch, y: y_val_batch})\n",
    "            print('step: ', step_idx)\n",
    "            #print('predicted scores: ', scores_val)\n",
    "            #print('true class', y_batch)\n",
    "            #print('true class predicted probability', y_pred_val)\n",
    "            print('val accuracy: ', val_acc)\n",
    "            print('test accuracy: ', test_acc)\n",
    "            print('----------------------------------')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdeeplearning",
   "language": "python",
   "name": "tfdeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
